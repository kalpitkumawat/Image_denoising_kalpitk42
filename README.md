# Open project -> Image Denoising 

( psnr 28.12 )

Paper Implemented -> Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement
The DCE network is built with multiple convolutional layers followed by concatenation layers to combine features from different stages capturing both fine-grained and high-level features, The use of 'relu' activations in the intermediate layers helps in learning complex patterns.
it learns to map the input low-light image to its best-fit curve parameters maps. The network consist of 7 convolution layers with symmetrical skip concatenation. First 6 convolution layers consist of 32 filters each with kernel size of 3x3 with stride of 1 followed by RelU activation. The last convolution layer has iteration x 3 number of filters (if we set iteration to 8 it will produce 24 curve parameters maps for 8 iteration, where each iteration generates three curve parameter maps for the three RGB channels.
It does not require any input/output image pairs during training. This is achieved through a set of carefully formulated non-reference loss functions, which implicitly measure the enhancement quality and guide the training of the network. The paper proposes set of zero-reference loss functions that differentiable which allows to assess the quality of enhanced image. They implicitly measure the enhancement quality and drive the learning of the network.

1.	Spatial Consistency Loss Class : By ensuring consistency of the spatial feature maps produced over consecutive training epochs, maintaining per-class running-average heatmaps for each training image. We show that this spatial consistency loss further improves The enhanced image through preserving the difference of neighbouring regions between the input image and its enhanced version. Loss will be computed per batch sample without reducing across the batch. Also kernels are used to compute differences in the spatial consistency between original and enhanced image. We have used average pooling to reduce the spatial dimensions. We have computed spatial differences between adjacent pixels using convolution to preserve the difference between neighbouring pixels. This class measures how well the enhanced image preserves the spatial arrangement of illumination from the original image.
2.	Exposure Control Loss Class :  Only when the exposure is properly set, can the colour and details of the images be appropriately preserved. For the under/over exposed regions ,to control the exposure  level. The exposure control loss measures the distance between the average intensity value of a local region to the well-exposedness level E. The value of E is empirically set to 0.6 in the paper. Computes the mean intensity across the RGB channels used average pooling to compute the mean Computes the exposure loss and helps in adjusting it across the image.
3.	Illumination Smoothness Loss Class : To preserve the relations between neighbouring pixels, the illumination smoothness loss is added to each curve parameter map. count_h and count_w are total number of horizontal and vertical pairwise differences . Compute total loss as sum of squared differences between horizontally and vertically adjacent pixels for each image of batch.
4.	Colour Constancy Loss Class : Colour constancy refers to the ability to perceive colours as relatively constant over varying illuminations. The purpose of Colour constancy loss to correct the potential colour deviations in the enhanced image and also build the relations among the three adjusted channels. We compute the mean RGB values across the image batch and computes the colour constancy loss as sqrt of the squared mean RGB values.

Dataset Preparation :  The dataset on which the training is done is the Train dataset which contains approx. 789 images and we have used about 400 for training and remaining for testing. Each image pair in Train dataset consists of a low-light input image and its corresponding well-exposed reference image. We have split the images available for training into train and validation sets with the train dataset containing 400 images and the validation dataset containing remaining images. The Adam optimizer is used for the optimization of the training process and the learning rate being 2 Ã— 10-4 . Model is train in batch of size 16. The model was trained for 40 epochs.

In this script, We have evaluate the performance of an image enhancement algorithm on a subset of low-light images by calculating and displaying their PSNR (Peak Signal-to-Noise Ratio) values. You select the first 25 images from your test dataset and their corresponding high-light reference images. For each image pair, you load the original low-light image, enhance it using the enhance image function, and load the corresponding high-light image. These images are then converted to grayscale if necessary for PSNR calculation. The PSNR value between the enhanced image and the high-light image is computed using the compute psnr function and stored for averaging. The display images function visually presents the original, enhanced, and high-light images side by side, along with the PSNR value for the enhanced image

This approach provides both visual and quantitative assessments of the enhancement quality, demonstrating the algorithm's effectiveness in improving low-light images. By creating high-order tonal curves tailored to each individual image, Zero-DCE significantly boosts image quality, colour accuracy, and overall clarity. This method uniquely approaches enhancement by estimating curves specific to the lighting conditions of each image, addressing the unique challenges presented by low-light scenarios. Moreover, Zero-DCE is highly versatile and efficient, as it does not require paired training data, making it suitable for a wide range of low-light situations. The practical applications of Zero-DCE are broad and impactful. It can be utilized in various fields such as photography, surveillance, medical imaging, and any domain where image clarity under low-light conditions is critical.

Results: The model is evaluated mainly on the metrics PSNR (peak signal-to-noise ratio) which comes out to be close to 28.12 in our case.

https://arxiv.org/pdf/2001.06826

https://www.youtube.com/watch?v=dJYGatp4SvA&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r

https://youtu.be/2xqkSUhmmXU?si=gjikUbQkxTE57-md


