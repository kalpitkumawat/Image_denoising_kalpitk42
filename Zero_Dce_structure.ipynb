{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY88y0qx32rQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import os\n",
        "from PIL import Image, ImageOps\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import shutil\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aoHisTNuDF-"
      },
      "outputs": [],
      "source": [
        "def exposure_loss(image, average_val=0.6):\n",
        "    image = tf.reduce_mean(image, axis=3, keepdims=True)\n",
        "    average = tf.nn.avg_pool2d(image, ksize=16, strides=16, padding=\"VALID\")\n",
        "    return tf.reduce_mean(tf.square(average - average_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD52WkxQ4jDn"
      },
      "outputs": [],
      "source": [
        "def illumination_smoothness_loss(img):\n",
        "    batch_size = tf.shape(img)[0]\n",
        "    height = tf.shape(img)[1]\n",
        "    width = tf.shape(img)[2]\n",
        "    count_horizontal = (tf.shape(img)[2] - 1) * tf.shape(img)[3]\n",
        "    count_vertical = tf.shape(img)[2] * (tf.shape(img)[3] - 1)\n",
        "    tv_horizontal = tf.reduce_sum(tf.square(img[:, 1:, :, :] - img[:, :height-1, :, :]))\n",
        "    tv_vertical = tf.reduce_sum(tf.square(img[:, :, 1:, :] - img[:, :, :width-1, :]))\n",
        "    batch_size = tf.cast(batch_size, dtype=tf.float32)\n",
        "    count_horizontal = tf.cast(count_horizontal, dtype=tf.float32)\n",
        "    count_vertical = tf.cast(count_vertical, dtype=tf.float32)\n",
        "    return 2 * (tv_horizontal / count_horizontal + tv_vertical / count_vertical) / batch_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhWZwPS94i8q"
      },
      "outputs": [],
      "source": [
        "class SpatialConsistencyLossModified(tf.keras.losses.Loss):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SpatialConsistencyLossModified, self).__init__(reduction=\"none\")\n",
        "\n",
        "        self.left_filter = tf.constant([[[[0, 0, 0]], [[-1, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32)\n",
        "        self.right_filter = tf.constant([[[[0, 0, 0]], [[0, 1, -1]], [[0, 0, 0]]]], dtype=tf.float32)\n",
        "        self.up_filter = tf.constant([[[[0, -1, 0]], [[0, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32)\n",
        "        self.down_filter = tf.constant([[[[0, 0, 0]], [[0, 1, 0]], [[0, -1, 0]]]], dtype=tf.float32)\n",
        "\n",
        "    def call(self, true_values, predicted_values):\n",
        "        original_mean = tf.reduce_mean(true_values, 3, keepdims=True)\n",
        "        enhanced_mean = tf.reduce_mean(predicted_values, 3, keepdims=True)\n",
        "        original_pool = tf.nn.avg_pool2d(original_mean, ksize=4, strides=4, padding=\"VALID\")\n",
        "        enhanced_pool = tf.nn.avg_pool2d(enhanced_mean, ksize=4, strides=4, padding=\"VALID\")\n",
        "\n",
        "        d_original_left = tf.nn.conv2d(original_pool, self.left_filter, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "        d_original_right = tf.nn.conv2d(original_pool, self.right_filter, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "        d_original_up = tf.nn.conv2d(original_pool, self.up_filter, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "        d_original_down = tf.nn.conv2d(original_pool, self.down_filter, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "\n",
        "        d_enhanced_left = tf.nn.conv2d(enhanced_pool, self.left_filter, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "        d_enhanced_right = tf.nn.conv2d(enhanced_pool, self.right_filter, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "        d_enhanced_up = tf.nn.conv2d(enhanced_pool, self.up_filter, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "        d_enhanced_down = tf.nn.conv2d(enhanced_pool, self.down_filter, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "\n",
        "        diff_left = tf.square(d_original_left - d_enhanced_left)\n",
        "        diff_right = tf.square(d_original_right - d_enhanced_right)\n",
        "        diff_up = tf.square(d_original_up - d_enhanced_up)\n",
        "        diff_down = tf.square(d_original_down - d_enhanced_down)\n",
        "\n",
        "        return diff_left + diff_right + diff_up + diff_down\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PqxZz434jQu"
      },
      "outputs": [],
      "source": [
        "def color_constancy_loss(img):\n",
        "    mean_rgb = tf.reduce_mean(img, axis=(1, 2), keepdims=True)\n",
        "    mean_red, mean_green, mean_blue = mean_rgb[:, :, :, 0], mean_rgb[:, :, :, 1], mean_rgb[:, :, :, 2]\n",
        "    diff_red_green = tf.square(mean_red - mean_green)\n",
        "    diff_red_blue = tf.square(mean_red - mean_blue)\n",
        "    diff_green_blue = tf.square(mean_green - mean_blue)\n",
        "    return tf.sqrt(tf.square(diff_red_green) + tf.square(diff_red_blue) + tf.square(diff_green_blue))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahVcdZpk32iN"
      },
      "outputs": [],
      "source": [
        "def build_dce_network():\n",
        "    input_image = tf.keras.Input(shape=[None, None, 3])\n",
        "    conv_layer1 = layers.Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(input_image)\n",
        "    conv_layer2 = layers.Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(conv_layer1)\n",
        "    conv_layer3 = layers.Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(conv_layer2)\n",
        "    conv_layer4 = layers.Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(conv_layer3)\n",
        "\n",
        "    concat1 = layers.Concatenate(axis=-1)([conv_layer4, conv_layer3])\n",
        "    conv_layer5 = layers.Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(concat1)\n",
        "    concat2 = layers.Concatenate(axis=-1)([conv_layer5, conv_layer2])\n",
        "    conv_layer6 = layers.Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(concat2)\n",
        "    concat3 = layers.Concatenate(axis=-1)([conv_layer6, conv_layer1])\n",
        "    output_layer = layers.Conv2D(24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\")(concat3)\n",
        "    return tf.keras.Model(inputs=input_image, outputs=output_layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us5UOGNe4iz_"
      },
      "outputs": [],
      "source": [
        "class ZeroDCE(tf.keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ZeroDCE, self).__init__(**kwargs)\n",
        "        self.dce_net = build_dce_network()\n",
        "\n",
        "    def compile(self, learning_rate, **kwargs):\n",
        "        super(ZeroDCE, self).compile(**kwargs)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        self.spatial_constancy_loss = SpatialConsistencyLossModified(reduction=\"none\")\n",
        "\n",
        "    def enhance_image(self, data, output):\n",
        "        r1, r2, r3, r4, r5, r6, r7, r8 = tf.split(output, num_or_size_splits=8, axis=-1)\n",
        "        x = data + r1 * (tf.square(data) - data)\n",
        "        x = x + r2 * (tf.square(x) - x)\n",
        "        x = x + r3 * (tf.square(x) - x)\n",
        "        x = x + r4 * (tf.square(x) - x)\n",
        "        x = x + r5 * (tf.square(x) - x)\n",
        "        x = x + r6 * (tf.square(x) - x)\n",
        "        x = x + r7 * (tf.square(x) - x)\n",
        "        enhanced_image = x + r8 * (tf.square(x) - x)\n",
        "        return enhanced_image\n",
        "\n",
        "    def call(self, inputs):\n",
        "        dce_net_output = self.dce_net(inputs)\n",
        "        return self.enhance_image(inputs, dce_net_output)\n",
        "\n",
        "    def compute_losses(self, inputs, outputs):\n",
        "        enhanced_image = self.enhance_image(inputs, outputs)\n",
        "        loss_illumination = 200 * illumination_smoothness_loss(outputs)\n",
        "        loss_spatial_constancy = tf.reduce_mean(self.spatial_constancy_loss(enhanced_image, inputs))\n",
        "        loss_color_constancy = 5 * tf.reduce_mean(color_constancy_loss(enhanced_image))\n",
        "        loss_exposure = 10 * tf.reduce_mean(exposure_loss(enhanced_image))\n",
        "        total_loss = (\n",
        "            loss_illumination\n",
        "            + loss_spatial_constancy\n",
        "            + loss_color_constancy\n",
        "            + loss_exposure\n",
        "        )\n",
        "        return {\n",
        "            \"total_loss\": total_loss,\n",
        "            \"illumination_smoothness_loss\": loss_illumination,\n",
        "            \"spatial_constancy_loss\": loss_spatial_constancy,\n",
        "            \"color_constancy_loss\": loss_color_constancy,\n",
        "            \"exposure_loss\": loss_exposure,\n",
        "        }\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = self.dce_net(data)\n",
        "            losses = self.compute_losses(data, outputs)\n",
        "        gradients = tape.gradient(losses[\"total_loss\"], self.dce_net.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.dce_net.trainable_weights))\n",
        "        return losses\n",
        "\n",
        "    def test_step(self, data):\n",
        "        outputs = self.dce_net(data)\n",
        "        return self.compute_losses(data, outputs)\n",
        "\n",
        "    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n",
        "        \"\"\"Save the weights of the DCE-Net.\"\"\"\n",
        "        self.dce_net.save_weights(filepath, overwrite=overwrite, save_format=save_format, options=options)\n",
        "\n",
        "    def load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None):\n",
        "        \"\"\"Load the weights of the DCE-Net.\"\"\"\n",
        "        self.dce_net.load_weights(filepath=filepath, by_name=by_name, skip_mismatch=skip_mismatch, options=options)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}